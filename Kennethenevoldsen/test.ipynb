{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, cuda \n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "import Dataloader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KennethEnevoldsen/dfm-sentence-encoder-large were not used when initializing BertModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.transform.LayerNorm.bias', 'lm_head.transform.LayerNorm.weight', 'lm_head.transform.dense.bias', 'lm_head.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Men forsikringsoplysningen, Anja, hvad er det, I tilbyder der?', ' Vi er en del af Forsikring- og pensionsselskabernes brancheorganisation, og vi er en rådgivningstjeneste, hvor man kan ringe ind og stille spørgsmål til sine forsikringer. F.eks. hvis man har været ude for et uheld, eller hvis man står over for at købe en ny forsikring.  Men det er faktisk også et sted, hvor man kan få hjælp til at sammenligne priser og tægning på forskellige forsikringer. Vi har et værktøj, der hedder forsikringsguiden.dk, og der kan man simpelthen sammenligne en lang række forsikringer. Og man kan også få hjælp til ligesom at afklare, hvad er det for forsikringer, jeg reelt har behov for. Så vi er ikke et forsikringssatskab, men vi er en tjeneste, som ligesom hjælper dig til at træffe nogle valg.')\n"
     ]
    }
   ],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# Loads data and returns a list of sentences as specfies in Dataloader\n",
    "sentences = dl.loaddata()\n",
    "\n",
    "# Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KennethEnevoldsen/dfm-sentence-encoder-large\")\n",
    "model = AutoModel.from_pretrained(\"KennethEnevoldsen/dfm-sentence-encoder-large\")\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(\n",
    "    sentences, padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "\n",
    "sentence_embeddings = sentence_embeddings.detach().numpy()\n",
    "\n",
    "# Perform t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=0)\n",
    "# tsne_results = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# PCA Prep\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(sentence_embeddings)\n",
    "\n",
    "# Perform UMAP Prep\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "X_umap = reducer.fit_transform(sentence_embeddings)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Index sentences along with their embeddings\n",
    "sentence_index = {}\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    sentence_index[sentence] = embedding\n",
    "\n",
    "def retrieve_answer(question, threshold=0.7):\n",
    "    # Tokenize the question and compute its embedding\n",
    "    encoded_question = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        question_output = model(**encoded_question)\n",
    "    question_embedding = mean_pooling(question_output, encoded_question[\"attention_mask\"]).numpy()\n",
    "\n",
    "    # Reshape the question embedding to 2D array\n",
    "    question_embedding = np.reshape(question_embedding, (1, -1))\n",
    "\n",
    "    # Calculate cosine similarity between the question embedding and all sentence embeddings\n",
    "    similarities = {}\n",
    "    for sentence, embedding in sentence_index.items():\n",
    "        # Reshape the sentence embedding to 2D array\n",
    "        embedding = np.reshape(embedding, (1, -1))\n",
    "        similarity = cosine_similarity(question_embedding, embedding)[0][0]\n",
    "        similarities[sentence] = similarity\n",
    "\n",
    "    # Find the most similar sentence\n",
    "    most_similar_sentence = max(similarities, key=similarities.get)\n",
    "    similarity_score = similarities[most_similar_sentence]\n",
    "\n",
    "    # If similarity score is above the threshold, return the answer associated with the sentence\n",
    "    if similarity_score >= threshold:\n",
    "        return most_similar_sentence\n",
    "    else:\n",
    "        return \"I'm sorry, I don't have an answer to that question.\"\n",
    "\n",
    "# Example usage:\n",
    "question = \"Men forsikringsoplysningen, Anja, hvad er det, I tilbyder der?\"\n",
    "answer = retrieve_answer(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Dataloader as dl\n",
    "\n",
    "sentences = dl.loaddata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"KennethEnevoldsen/dfm-sentence-encoder-large\")\n",
    "model = AutoModel.from_pretrained(\"KennethEnevoldsen/dfm-sentence-encoder-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "# plt.scatter(tsne_results[:,0], tsne_results[:,1])\n",
    "# plt.title('t-SNE of Sentence Embeddings')\n",
    "# plt.xlabel('Dimension 1')\n",
    "# plt.ylabel('Dimension 2')\n",
    "# plt.show()\n",
    "\n",
    "# Plot PCA\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Plot of Question-Answer Pairs')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot UMAP\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1])\n",
    "plt.title('UMAP plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.xticks(np.arange(1, len(pca.explained_variance_ratio_) + 1))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #VERY OUT DATED\n",
    "# def answer_question(question, context):\n",
    "#     # Tokenize the input\n",
    "#     inputs = tokenizer.encode_plus(question, context, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "#     # Get the model's predictions\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "#     # Get the start and end scores\n",
    "#     start_scores, end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "#     # Get the start and end positions\n",
    "#     start_idx = torch.argmax(start_scores)\n",
    "#     end_idx = torch.argmax(end_scores)\n",
    "\n",
    "#     # Get the answer\n",
    "#     answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx:end_idx+1]))\n",
    "\n",
    "#     return answer\n",
    "\n",
    "# context = 'insurance is a means of protection from financial loss. It is a form of risk management, primarily used to hedge against the risk of a contingent or uncertain loss. An entity which provides insurance is known as an insurer, an insurance company, an insurance carrier or an underwriter. A person or entity who buys insurance is known as an insured or as a policyholder. The insurance transaction involves the insured assuming a guaranteed and known relatively small loss in the form of payment to the insurer in exchange for the insurer is a promise to compensate the insured in the case of a financial loss. The insured receives a contract, called the insurance policy, which details the conditions and circumstances under which the insurer will compensate the insured. The amount of money charged by the insurer to the policyholder for the coverage set forth in the insurance policy is called the premium. If the insured experiences a loss which is potentially covered by the insurance policy, the insured submits a claim to the insurer for processing by a claims adjuster.'\n",
    "# question = \"What is insurance?\"\n",
    "# print(answer_question(question, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Index sentences along with their embeddings\n",
    "sentence_index = {}\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    sentence_index[sentence] = embedding\n",
    "\n",
    "def retrieve_answer(question, threshold=0.7):\n",
    "    # Tokenize the question and compute its embedding\n",
    "    encoded_question = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        question_output = model(**encoded_question)\n",
    "    question_embedding = mean_pooling(question_output, encoded_question[\"attention_mask\"]).numpy()\n",
    "\n",
    "    # Reshape the question embedding to 2D array\n",
    "    question_embedding = np.reshape(question_embedding, (1, -1))\n",
    "\n",
    "    # Calculate cosine similarity between the question embedding and all sentence embeddings\n",
    "    similarities = {}\n",
    "    for sentence, embedding in sentence_index.items():\n",
    "        # Reshape the sentence embedding to 2D array\n",
    "        embedding = np.reshape(embedding, (1, -1))\n",
    "        similarity = cosine_similarity(question_embedding, embedding)[0][0]\n",
    "        similarities[sentence] = similarity\n",
    "\n",
    "    # Find the most similar sentence\n",
    "    most_similar_sentence = max(similarities, key=similarities.get)\n",
    "    similarity_score = similarities[most_similar_sentence]\n",
    "\n",
    "    # If similarity score is above the threshold, return the answer associated with the sentence\n",
    "    if similarity_score >= threshold:\n",
    "        return most_similar_sentence\n",
    "    else:\n",
    "        return \"I'm sorry, I don't have an answer to that question.\"\n",
    "\n",
    "# Example usage:\n",
    "question = \"I tilbyder der, hvad?\"\n",
    "answer = retrieve_answer(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "def ask_gpt3(question, context):\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"text-davinci-002\",\n",
    "      prompt=f\"{context}\\n{question}\",\n",
    "      temperature=0.5,\n",
    "      max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "context = \"The sky is blue during the day because of the way Earth's atmosphere scatters sunlight in all directions. More blue light is scattered than other colors because it travels in smaller, shorter waves. This is known as Rayleigh scattering.\"\n",
    "\n",
    "question = \"Why is the sky blue?\"\n",
    "\n",
    "print(ask_gpt3(question, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is isaac newton?\n",
      "\n",
      "I'm not sure if you can say that.\n",
      "\n",
      "I'm not sure if you can say that.\n",
      "\n",
      "I'm not sure if you can say that.\n",
      "\n",
      "I'm not sure if you can say that.\n",
      "\n",
      "I'm not sure if you can say that.\n",
      "\n",
      "I'm not sure if you can say that.\n",
      "\n",
      "I'm not sure if you can say that.\n",
      "\n",
      "I'm not sure if you\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def ask_gpt2(question):\n",
    "    inputs = tokenizer.encode_plus(question, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], \n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=100, \n",
    "        num_return_sequences=1, \n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    answer = tokenizer.decode(outputs[0])\n",
    "    return answer\n",
    "\n",
    "question = \"who is isaac newton?\"\n",
    "\n",
    "print(ask_gpt2(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, EncoderDecoderModel\n",
    "# import torch\n",
    "\n",
    "# # Initialize the tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Initialize the model\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')\n",
    "\n",
    "# # Define the training data\n",
    "# sentences = [(question, answers) for question, answers in sentences]\n",
    "\n",
    "# # Separate questions and answers\n",
    "# questions, answers = zip(*sentences)\n",
    "\n",
    "# # Tokenize the training data\n",
    "# inputs = tokenizer(list(questions), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# outputs = tokenizer(list(answers), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# # Define the model's parameters\n",
    "# model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "# model.config.eos_token_id = tokenizer.sep_token_id\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.vocab_size = model.config.encoder.vocab_size\n",
    "\n",
    "# # Train the model\n",
    "# model.train()\n",
    "# outputs = model(input_ids=inputs[\"input_ids\"], decoder_input_ids=outputs[\"input_ids\"], labels=outputs[\"input_ids\"])\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# # Perform backpropagation\n",
    "# loss = outputs.loss\n",
    "# loss.backward()\n",
    "\n",
    "# # Update the weights\n",
    "# optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
