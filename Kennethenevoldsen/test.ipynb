{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, cuda \n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "import Dataloader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KennethEnevoldsen/dfm-sentence-encoder-large were not used when initializing BertModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.transform.LayerNorm.bias', 'lm_head.transform.LayerNorm.weight', 'lm_head.transform.dense.bias', 'lm_head.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\RaChr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\umap\\umap_.py:2433: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# Loads data and returns a list of sentences as specfies in Dataloader\n",
    "sentences = dl.loaddata()\n",
    "\n",
    "\n",
    "# Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KennethEnevoldsen/dfm-sentence-encoder-large\")\n",
    "model = AutoModel.from_pretrained(\"KennethEnevoldsen/dfm-sentence-encoder-large\")\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(\n",
    "    sentences, padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "\n",
    "sentence_embeddings = sentence_embeddings.detach().numpy()\n",
    "\n",
    "# Perform t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=0)\n",
    "# tsne_results = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(sentence_embeddings)\n",
    "\n",
    "# Perform UMAP\n",
    "reducer = umap.UMAP()\n",
    "X_umap = reducer.fit_transform(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Dataloader as dl\n",
    "\n",
    "sentences = dl.loaddata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KennethEnevoldsen/dfm-sentence-encoder-large were not used when initializing BertModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.transform.LayerNorm.bias', 'lm_head.transform.LayerNorm.weight', 'lm_head.transform.dense.bias', 'lm_head.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"KennethEnevoldsen/dfm-sentence-encoder-large\")\n",
    "model = AutoModel.from_pretrained(\"KennethEnevoldsen/dfm-sentence-encoder-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tsne_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mtsne_results\u001b[49m[:,\u001b[38;5;241m0\u001b[39m], tsne_results[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt-SNE of Sentence Embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDimension 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tsne_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the results\n",
    "# plt.scatter(tsne_results[:,0], tsne_results[:,1])\n",
    "# plt.title('t-SNE of Sentence Embeddings')\n",
    "# plt.xlabel('Dimension 1')\n",
    "# plt.ylabel('Dimension 2')\n",
    "# plt.show()\n",
    "\n",
    "# Plot PCA\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.title('PCA plot')\n",
    "\n",
    "# Plot UMAP\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1])\n",
    "plt.title('UMAP plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "    # Get the model's predictions\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the start and end scores\n",
    "    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "    # Get the start and end positions\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the answer\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx:end_idx+1]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "context = 'insurance is a means of protection from financial loss. It is a form of risk management, primarily used to hedge against the risk of a contingent or uncertain loss. An entity which provides insurance is known as an insurer, an insurance company, an insurance carrier or an underwriter. A person or entity who buys insurance is known as an insured or as a policyholder. The insurance transaction involves the insured assuming a guaranteed and known relatively small loss in the form of payment to the insurer in exchange for the insurer is a promise to compensate the insured in the case of a financial loss. The insured receives a contract, called the insurance policy, which details the conditions and circumstances under which the insurer will compensate the insured. The amount of money charged by the insurer to the policyholder for the coverage set forth in the insurance policy is called the premium. If the insured experiences a loss which is potentially covered by the insurance policy, the insured submits a claim to the insurer for processing by a claims adjuster.'\n",
    "question = \"What is insurance?\"\n",
    "print(answer_question(question, context))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
